{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/nora/Box/dssg/DrivenData/Heart/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + 'test_values.csv')\n",
    "train = pd.read_csv(data_dir + 'train_values.csv')\n",
    "labels = pd.read_csv(data_dir + 'train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npatients = len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_columns(df, cols=[]):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return df\n",
    "        \n",
    "def standardize(df, numeric_only=True):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    df[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pre_process_data(df, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(df.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(df)\n",
    "    print(\"After standardization {}\".format(df.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    print(\"After converting categoricals:\\t{}\".format(df.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patient_id = train['patient_id'].as_matrix()\n",
    "train = train.drop('patient_id', axis=1)\n",
    "train = convert_columns(train, cols=['chest_pain_type', 'resting_ekg_results'])\n",
    "train = pre_process_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.ravel(labels.drop('patient_id', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient_id = test['patient_id'].as_matrix()\n",
    "test = test.drop('patient_id', axis=1)\n",
    "test = convert_columns(test, cols=['chest_pain_type', 'resting_ekg_results'])\n",
    "test = pre_process_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cutoff = int(npatients*0.75)\n",
    "# train1 = train.iloc[:cutoff]\n",
    "# train2 = train.iloc[cutoff:]\n",
    "# labels1 = labels[:cutoff]\n",
    "# labels2 = labels[cutoff:]\n",
    "# print train1.shape, train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1, train2, labels1, labels2 = train_test_split(train, labels, train_size=0.75, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_submission(model, test, fname='submission.csv'):\n",
    "    probs = model.predict_proba(test)\n",
    "    sub = pd.DataFrame(np.vstack([test_patient_id, probs[:,1]]).T)\n",
    "    sub.to_csv(fname, sep=',', header=['patient_id','heart_disease_present'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_score(model, test, labels, printing=True):\n",
    "    probs = model.predict_proba(test)\n",
    "    preds = model.predict(test)\n",
    "    if printing:\n",
    "        print 'Accuracy: %.2f' %(model.score(test, labels)) # %(len(np.where(preds == labels))/len(preds))\n",
    "        print 'Log loss: %.2f' %(log_loss(labels, probs))\n",
    "    return log_loss(labels, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forests are often a good model to try first, especially when we have numeric and categorical variables in our feature space.\n",
    "def train_etc(features, labels, **kwargs):\n",
    "    \n",
    "    # instantiate model\n",
    "    # model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "    model = ExtraTreesClassifier(n_estimators=50, random_state=0)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(features, labels)\n",
    "    \n",
    "    # get a (not-very-useful) sense of performance\n",
    "    accuracy = model.score(features, labels)\n",
    "    # print(f\"In-sample accuracy: {accuracy:0.2%}\")\n",
    "    # print(\"In-sample accuracy: %.2f percent\" %(accuracy*100))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a = train_etc(train1, labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model_a.predict(train2)\n",
    "probs = model_a.predict_proba(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Accuracy: %.2f' %(len(np.where(preds == labels2))/len(preds))\n",
    "print 'Log loss: %.2f' %(log_loss(labels2, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_b = train_etc(train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = model_b.predict_proba(test)\n",
    "preds = model_b.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(np.vstack([test_patient_id, probs[:,1]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission1.csv', sep=',', header=['patient_id','heart_disease_present'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_submission(model_b, test, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forests are often a good model to try first, especially when we have numeric and categorical variables in our feature space.\n",
    "def train_rfc(features, labels, **kwargs):\n",
    "    \n",
    "    # instantiate model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(features, labels)\n",
    "    \n",
    "    # get a (not-very-useful) sense of performance\n",
    "    accuracy = model.score(features, labels)\n",
    "    # print(f\"In-sample accuracy: {accuracy:0.2%}\")\n",
    "    # print(\"In-sample accuracy: %.2f percent\" %(accuracy*100))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train_rfc(train1, labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = get_score(model, train2, labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50, random_state=12)\n",
    "rfe = RFE(rfc, n_features_to_select=13)\n",
    "rfe = rfe.fit(train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "idx_rank = np.argsort(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(rfe.support_==False)[0]\n",
    "cols = train.keys()\n",
    "cols[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(cols[idx_rank]):\n",
    "    print rfe.ranking_[idx_rank][i], c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, rfe.support_].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train_rfc(train1.loc[:, rfe.support_], labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(model, train2.loc[:, rfe.support_], labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train_rfc(train.loc[:, rfe.support_], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_submission(model, test.loc[:, rfe.support_], 'submission4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmin = np.inf\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=12)\n",
    "for n_feat in range(1,train.shape[1]+1):\n",
    "    print\n",
    "    rfe = RFE(rfc, n_features_to_select=n_feat)\n",
    "    rfe = rfe.fit(train, labels)\n",
    "    model = train_rfc(train1.loc[:, rfe.support_], labels1)\n",
    "    logloss = get_score(model, train2.loc[:, rfe.support_], labels2)\n",
    "    if logloss < llmin:\n",
    "        llmin = logloss\n",
    "        print 'New best score with n_feat = %i' %n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmin = np.inf\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=12)\n",
    "for n_feat in range(1,train.shape[1]+1):\n",
    "    print 'n_feat = %i' %n_feat\n",
    "    score = 0.\n",
    "    for i in range(10):\n",
    "        # print score\n",
    "        train1, train2, labels1, labels2 = train_test_split(train, labels, train_size=0.75, test_size=0.25, shuffle=True)\n",
    "        # print\n",
    "        rfe = RFE(rfc, n_features_to_select=n_feat)\n",
    "        rfe = rfe.fit(train1, labels1)\n",
    "        model = train_rfc(train1.loc[:, rfe.support_], labels1)\n",
    "        logloss = get_score(model, train2.loc[:, rfe.support_], labels2, printing=False)\n",
    "        score += logloss\n",
    "    print score/11.\n",
    "    if score < llmin:\n",
    "        llmin = score\n",
    "        print 'New best score with n_feat = %i' %n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = TSNE(n_components=2).fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='none', vmax=1.5, s=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "params = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "logreg_gs = GridSearchCV(logreg, params, cv=10, return_train_score=True)\n",
    "logreg_gs.fit(train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Best params: ', logreg_gs.best_params_\n",
    "best_lr = logreg_gs.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print 'Best number of features: ', coefs.size\n",
    "print 'Number of selected features: %i' %np.count_nonzero(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Best accuracy: %.2f' %(logreg_gs.best_score_ * 100)\n",
    "print 'Best parameters: ', logreg_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_submission(logreg_gs, test, fname='submission_logreg_gs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
